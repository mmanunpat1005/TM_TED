---
title: "Embedding"
author: "Ting Yang"
date: "09/12/2022"
output: html_document
---

```{r,include=FALSE}
source(here::here("scripts/setup.R"))
```

# 7. Embedding 

In addition to using Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA), we also plan to utilize word and document embeddings to analyze the transcripts of TED videos. Embedding refers to the representation of elements (e.g., documents or tokens) in a vector space model, where words are first embedded and then document embeddings are constructed that capture the co-occurrence patterns of the words within the document.

## 7.1 Word Embedding

The objective of word embedding is to learn a representation of words that reflects their co-occurrence patterns. To obtain these co-occurrence patterns, we utilize the fcm function from the quanteta package. 

Below, we present a sample of the resulting co-occurrence data. As can be seen, the co-occurrence between the words *artificial* and *intelligence* is relatively high (170), while the co-occurrence between the words *fly* and *intelligence* is considerably lower (0). These patterns are indicative of the relationships between the words in the corpus and will be useful in constructing the word embeddings.

```{r, echo=TRUE, warning=FALSE}
TED.coo <- fcm(TED.tk,
               context = "window",
               window = 5,
               tri = FALSE) 

TC <- head(TED.coo) %>% 
  convert(to="data.frame") %>% 
  select(1:6)

kable(TC,
      caption = "Co-occurrence matrix") %>%
   kable_paper() %>%
   kableExtra::scroll_box(width = "100%", height = "200px")
```

```{r, echo=TRUE, warning=FALSE,results="hide"}
# RcppParallel::setThreadOptions(1)
set.seed(123)
p <- 2 # word embedding dimension
TED.glove <- GlobalVectors$new(rank = p,
                               x_max = 10) # x_max is a needed technical option
TED.we <- TED.glove$fit_transform(TED.coo, n_threads = 1) # central vectors; speech.glove$components contains the context vectors
TED.we <- t(TED.glove$components) + TED.we# unique representation
```

In order to visualize the learned word embeddings, we create two plots. The first plot depicts the vectors of the 100 most frequently used words (i.e., the 100 words with the largest frequencies). The second plot shows all of the words, but only label a subset of them. 

```{r, echo=TRUE, warning=FALSE,fig.height = 4}
index <- textstat_frequency(dfm(TED.tk))[1:100, ]$feature
## words with the 100 largest frequencies

data.for.plot <- data.frame(TED.we[index, ])
data.for.plot$word <- row.names(data.for.plot)

Emb_p1 <- ggplot(data.for.plot, 
       aes(x = X1,
           y = X2,
           label = word)) +
  geom_text_repel(max.overlaps = 100)+
  theme_void() +
  labs(title="Map of top100 words")

TED.we.df <- as.data.frame(TED.we)
word <- rownames(TED.we.df)
TED.we.df <- cbind(word,TED.we.df)
e <- c(1:15045)
row.names(TED.we.df) <- e

Emb_p2 <- ggplot(TED.we.df,aes(x=V1,y=V2))+
  geom_text_repel(data = subset(TED.we.df, V1 <=-1.8|V2>3|V1>2),
            mapping = aes(label = word),
            hjust = "inward",
            max.overlaps = 100) +
  geom_point(color="grey")+
  labs(title="Map of all words(partially labeled)")

Emb_p1

Emb_p2
```

To avoid label overlap between data points in the plots, we use the geom_text_repel function. Some labels are also accompanied by a black line, indicating the location of the corresponding data point.

The first plot depicts the relationships between frequently used words. It can be seen that words that are close in the embedding space are often used together. For example, the words "robot" and "computer" are close, indicating that they are frequently used together. Similarly, the words "man" and "woman" are close, suggesting that they are also commonly used together.

The second plot presents the distribution of all used words, with a subset labeled for illustration. This plot shows that words such as "carbon" and "emission" are close in the embedding space, indicating that they are often used together. 

## 7.2 Document Embedding

We now build the document embedding by computing the centroids of the documents.

```{r, echo=TRUE, warning=FALSE}
kable(TED.we[TED.tk[[1]], ],
      col.names = c("dimension1","dimension2"),
      caption = "Word vectors") %>%
   kable_paper() %>%
   kableExtra::scroll_box(width = "100%", height = "200px")
```


```{r, echo=TRUE, warning=FALSE}
nd <- length(TED.tk) # number of documents
TED.de <- matrix(nr = nd, nc = p) # document embedding matrix (1 document per row)
for(i in 1:nd) {
  words_in_i <- TED.we[TED.tk[[i]], , drop = FALSE] 
  # drop = FALSE is needed in case there is only one token
  TED.de[i, ] <- apply(words_in_i, 2 ,mean)
}
row.names(TED.de) <- names(TED.tk)

kable(TED.de,
      col.names = c("dimension1","dimension2"),
      caption = "Document vectors") %>%
   kable_paper() %>%
   kableExtra::scroll_box(width = "100%", height = "200px")

```
<br>
We present the representation of the documents and use different color to represent the document in different category.
```{r, echo=TRUE, warning=FALSE,fig.height = 4}
TED.de <- as.data.frame(TED.de)
TED.de.source <- TED_full %>% 
  select(2,3,4) %>% cbind(TED.de) 

ggplot(data=TED.de.source,mapping = aes(
  x=V1,
  y=V2,
  color=cate))+
  geom_point()+
  labs(x = "dimension1",
    y = "dimension2")+
  scale_colour_discrete(
    name="Category",
    breaks=c("1","2","3"),
    labels=c("AI","Climate change","Relationships")
  )
```
According to this plot, the documents in the "AI" and "relationships" categories covers the largest area of each other. It is possible that the documents in these two categories are more similar when compared with the documents in the "Climate change" category.


