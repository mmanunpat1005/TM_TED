---
title: "eda"
author: "Manunpat"
date: "2022-12-10"
output: html_document
---

```{r,include=FALSE}
source(here::here("scripts/setup.R"))
```

# 4. Exploratory Data Analysis  

In this section, we perform initial investigations on **TED** (transcript each video) to discover pattern and to spot anomalies with the summary statistics and graphical representations. We would like to present the analysis of the word frequency, the comparison of videos in term of lexical diversity, the comparison of videos in term of keyness, and the connections between terms by computing co-occurrence.

## 4.1 Analysis of the word frequency (plot of frequencies and TF-IDF)

```{r, echo=TRUE, warning=FALSE,fig.height = 3}

TED.freq1 %>% 
  top_n(20, frequency) %>%
  ggplot(aes(
    x = reorder(feature, frequency),
    y = frequency)) + 
  geom_bar(stat = "identity") +
  coord_flip() + #change x y axis
  ylab("Frequency") + 
  xlab("term") +
  ggtitle("The top 20 most frequent terms")

textplot_wordcloud(TED.dfm1)

```

From the top 20 most frequent terms chart and wordcloud plot, we can see that the top 5 terms, which are "people", "make", "thing", "time", and "year", are common terms. Due to 3 different topics, we do not expect to see topic specific terms in the top ranks of the most frequent used terms chart. However, we notice some terms that are related to our topics such as world, "love", "ai", and "kind". 

```{r,fig.height = 3}
TED.dfm1 %>% 
  tidy() %>% 
  top_n(5, count) %>% 
  ggplot(aes(x = term, y = count)) + 
  geom_bar(stat = "Identity") + 
  coord_flip() +
  theme(axis.text.y = element_text(size = 3.5),
        axis.ticks.y = element_blank())  + 
  facet_wrap(~document, ncol = 2)
```

We can associate the different texts with their most frequent terms. For example, text12 with “people”, “em”, and “ca” and "people" was expected since it is a common term, “em” and "ca" look more specific to this text.

Subsequently, we also would like to investigate the highest TF-IDF terms to observe the specific terms per document (video). Therefore, we present the top 10 highest TF-IDF in documents as shown below. For text12, we notice that "people" does not appear in the text12 anymore as expected. This is because the TF-IDF of "people" is very low since it is not specific to any text. On the other hand, *em* and *ca* are specific terms for text12. We then have a closer look to the text12 and discover the dialog between Elon Musk and Chris Anderson, so *em* and *ca* stand for Elon Musk and Chris Anderson, respectively.

```{r, echo=TRUE, warning=FALSE,fig.height = 3}

TED.tfidf1 %>% 
  tidy() %>% 
  top_n(5, count) %>% #may change to top 10
  ggplot(aes(x = term, y = count)) + 
  geom_col() + 
  coord_flip() + 
  theme(axis.text.y = element_text(size = 4),
        axis.ticks.y = element_blank())  + 
  facet_wrap(~document, ncol = 2)

```

To have an overall view of the terms with at least one large TF-IDF, we compute the max of the TF-IDF over all texts, for each term and present the top 20 highest TF-IDF. Regarding the below charts, we see that the terms *em* and *regret* have the largest weighted frequency, in the sense that the TF-IDF is large in at least one document. *em* and *regret* have TF-IDFs of over 200.

```{r, echo=TRUE, warning=FALSE,fig.height = 3}

#sort(apply(TED.tfidf1, 2, max), decreasing = TRUE)[1:10]

TED.tfidf1 %>% 
  tidy() %>%
  group_by(term) %>%
  summarize(count = max(count)) %>% #use summarize to find max
  ungroup() %>% 
  arrange(desc(count)) %>%
  top_n(20, count) %>%
  ggplot(aes(x=reorder(term, count),
             y = count)) + 
  geom_bar(stat = "identity") + 
  coord_flip() +
  xlab("Max TF-IDF") + 
  ylab("term")
```

## 4.2 Comparison of videos in term of Lexical Diversity

We perform lexical diversity analysis by presenting the **Type-Token Ratio or TTR**. The function **textstat_lexdiv()** from the quanteda.textstats package is used in the computation.

As we can see from the below chart and tables, the lexical diversity analysis is conclusive for these data. There are some texts which have very high TTR (approximately more than 0.8) meaning that those texts have richness of the vocabulary used in the texts. Then, the TTRs gradually decrease to the lowest TTR (approximately 0.3).

From the below tables, text237 and text131 have the highest richness of vocabulary among the other texts (videos), their TTRs are 0.816 and 0.802, respectively, while text88 has the lowest TTR (0.3306) meaning that text88 has the lowest diversity of the vocabulary used in the text compared to the other texts(videos) in this corpus(sample).

```{r, echo=TRUE, warning=FALSE,fig.height = 3}
ttr_top <- TED.dfm1 %>% textstat_lexdiv() %>% arrange(desc(TTR)) %>% top_n(10, TTR)
ttr_bottom <- TED.dfm1 %>% textstat_lexdiv() %>% arrange(desc(TTR)) %>% top_n(-10, TTR)

TED.dfm1 %>% textstat_lexdiv() %>% 
  ggplot(aes(reorder(document, -TTR),
             TTR)) + 
  geom_bar(stat="identity") +
  xlab("Text") +
  ggtitle("Type-Token Ratio per text")
 
kable(cbind(ttr_top,ttr_bottom), caption = "Text with highest and lowest TTR", caption.above = TRUE) %>%
  kable_paper() %>%  
  kableExtra::scroll_box(width = "100%", height = "200px")
```

## 4.3 Connections between terms by computing co-occurrence (Link between Words)

We start exploring the links between words by computing the co-occurrences of words in documents. The larger the value the more often two words occur together (in documents). For example, the value of "intelligence" and "artificial" are 1,208 which is quit high compared to the other values below. Hence, we can say that these two words often occur together. 

```{r, echo=TRUE, warning=FALSE}
TED.co <- fcm(TED.tk1, 
                context = "document", 
                tri = FALSE)

kable(TED.co[1:10,1:10], caption = "The example of co-occurrence matrix", caption.above = TRUE) %>%
  kable_paper() %>%  
  kableExtra::scroll_box(width = "100%", height = "200px")

```
<br>
To read the co-occurrences matrix comfortably, we restrict the analysis to the terms that have a frequency larger than 500.

```{r, echo=TRUE, warning=FALSE}
#create index = words that have frequency > 500
index <- TED.freq1 %>% 
  filter(frequency > 500) %>% 
  data.frame() %>% 
  select(feature)

#then refer them to co occurance table
x <- TED.co[index$feature, index$feature]
kable(x[1:10,1:10], caption = "The example of co-occurrence matrix after restricted the frequency larger than 500", caption.above = TRUE) %>%
  kable_paper() %>%  
  kableExtra::scroll_box(width = "100%", height = "200px")

```
<br>
We then use the igraph library to create a network object and plot it. Although we restrict the frequency of terms larger than 500, it is still difficult to read the igraph. Therefore, we decide that for less than 4500 co-occurences, there is no link (larger than 4500, there is one link).

From the igraph, we observe that *make*, *thing*, *people* are the central terms that co-occurs a lot with the others as they are not specific words to any texts. Again, due to 3 different topics, we do not expect to see the specific words for each topic in this analysis.

For *climate*, we can see it in the igraph without any links to the other words. This is because there is no co-occurance larger than 4500 however, its frequency is larger than 500.

```{r, echo=TRUE, warning=FALSE}
x[x <= 4500] <- 0
x[x > 4500] <- 1

network <- graph_from_adjacency_matrix(
  x,
  mode = "undirected",
  diag = FALSE)
plot(network,
     layout = layout_with_kk)

```



