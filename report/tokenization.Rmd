---
title: "Tokenization"
author: "Manunpat"
date: "2022-12-03"
output: html_document
---

```{r,include=FALSE}
source(here::here("scripts/setup.R"))
```

# 3. Tokenization
## 3.1 Wrangling and parsing data

After obtaining the transcript text from TED talk videos through web scraping and saving the scraped data in .csv format, we import the resulting data into our analysis. Specifically, we import two tables: "TED.csv" and "add_details_1.csv." The first table contains 330 observations and 11 variables, while the second table contains 310 observations and 2 variables. These tables are stored in a data folder and serve as the primary source of data for our analysis.

```{r, echo=TRUE, warning=FALSE}
# Import data
TED <- read_csv(here::here("data/TED.csv")) #330 obs
views_add <- read_csv(here::here("data/add_details_1.csv")) #310 obs

kable(TED[1:10,], caption = "The example of original TED table") %>%
   kable_paper() %>%
   kableExtra::scroll_box(width = "100%", height = "200px")

kable(views_add[1:10,], caption = "The example of original add_details_1 table") %>%
   kable_paper() %>%
   kableExtra::scroll_box(width = "100%", height = "200px")

```
<br>
We then remove any duplicated observations and combine the two tables by matching the "title" column. The resulting table, which we name **TED**, containing 324 observations and 12 variables. However, for the purpose of our analysis, we are only interested in six specific variables: title of videos (title), posting time (posted), topic of videos (cate), the number of likes for videos (likes), transcript (transcript), and the number of views of videos (views_details). Therefore, we select these variables and removed the rest, creating a new table called **TED_sentiment** that would serve as the main table for our sentiment analysis. We also remove the title variable from the **TED** table, as it would not be used in the sentiment analysis.

```{r, echo=TRUE, warning=FALSE}
# Delete duplicate rows of TED
#sum(duplicated(TED)) #6
TED <- TED[!duplicated(TED), ]

# Delete duplicate rows of views_add
#unique(views_add$page_title) #304 so duplicate title = 6
views_add <- views_add[!duplicated(views_add), ] 

# Combine tables
TED <- left_join(TED,views_add, by = c("title"="page_title")) 
TED <- TED %>% as_tibble(data.frame(TED)) %>%
  select(title, views_times.x, cate, likes, tanscript, views_details)
colnames(TED)[2] <- "posted"

# Identify NA and remove
# checkna <- TED[is.na(TED$tanscript), ]
# numtotal <- data.frame(table(TED$cate))
# numna <- data.frame(table(checkna$cate))
# diff <- numtotal %>% left_join(numna, by = "Var1") %>%
#   mutate(diff = Freq.x-Freq.y)
TED <- na.omit(TED) #286
catenum <- data.frame(table(TED$cate))
colnames(catenum) <- c("Topics", "Count")

kbl(catenum[,], caption = "The number of videos per topics") %>%
   kable_paper(bootstrap_options = "striped", full_width = F, position = "float_left") 

```

After performing these operations, we discover that the **TED** table containing 34 missing values (NAs), which we subsequently remove. The resulting **TED** table containing 286 observations, representing 103 videos on AI, 86 videos on Climate change, and 97 videos on Relationships. This table would serve as the basis for our further analyses.

Subsequently, it turns to data parsing step. We convert posting time, the number of likes, the number of views to be in the appropriate format for further analyses. For example, the posting time for the first video, *"How does artificial intelligence learn?"*, is Mar 2021 in the original TED table. It is converted to be 2021-03-01. 

For the transcript, there are the number of translated languages and the details of the translation at the beginning of the transcript every video. In this project, we focus only the actual transcript. Thus, we remove this part out of the transcript. For example, the first sentence of the transcript in the first video,*"How does artificial intelligence learn?"*, is "Transcript (28 Languages)Bahasa IndonesiaDeutschEnglishEspañolFrançaisItalianoMagyarPolskiPortuguês brasileiroPortuguês de PortugalRomânăTiếng ViệtTürkçeΕλληνικάРусскийСрпски, Srpskiעבריתالعربيةفارسىکوردی سۆرانیবাংলাதமிழ்ภาษาไทยမြန်မာဘာသာ中文 (简体)中文 (繁體)日本語한국어". We remove this part out of the transcript.

```{r, echo=TRUE, warning=FALSE}
# Parse data
TED$posted <- my(TED$posted)
TED$cate[which(TED$cate=="AI")] <- "1"
TED$cate[which(TED$cate=="Climate change")] <- "2"
TED$cate[which(TED$cate=="Relationships")] <- "3"
TED$likes <- gsub("[(]",'',TED$likes)
TED$likes <- gsub("[)]",'',TED$likes)
x <- substr(TED$likes[1],1,1)
TED$likes <- gsub(x,'',TED$likes)
TED$likes <- gsub("K", "e3", TED$likes)
TED$likes <- gsub("M", "e6", TED$likes)
TED$likes <- as.numeric(TED$likes)

# Clean number of views
# first separate the views detail into two parts (before "views" after "views")
views_time <- as.character()
for (i in 1:length(TED$views_details)) {
  views_temp <- TED$views_details[i]
  views_temp <- strsplit(views_temp, "views")
  views_temp <- views_temp[[1]][1]
  views_time <- append(views_time,views_temp)
}
views_time <- gsub(" ","",views_time)
views_time <- gsub(",","",views_time)
TED$views_details <- as.numeric(views_time)

# Clean transcript
TED$tanscript <- gsub("^.+?00:(.*)","\\1",TED$tanscript)
TED$tanscript <- gsub("\r\n"," ",TED$tanscript)
TED$tanscript <- gsub("[[:digit:]]"," ",TED$tanscript)

# extract on version of TED for sentiment part
TED_sentiment <- TED
# no need for title column in the following analysis
TED <- TED %>% select(-title)
```

In order to facilitate data analysis, we assign numerical values to the categories of AI, Climate Change, and Relationships in the TED dataset. The variable **cate** is used to represent these categories, with AI being represented by number 1, Climate Change by number 2, and Relationships by number 3. This allows for easier tracking of the videos in both supervised and unsupervised learning analyses.

Due to the limited number of available videos within the selected categories on the TED website, we are unable to gather a larger dataset for unsupervised and supervised learning analyses. In order to increase the number of observations while still avoiding overfitting and striving for a robust model, we decide to set a window of 20 sentences to be equal to one observation. This is based on the observation that the transcript for each video typically contains more than 20 sentences.

We split sentences by using the **tokenize_sentence()** function from the quanteda package and create a new variable, namely **sub cate**. For example, sub_cate of 1.1 indicates that the observation is from the first transcript in the AI category (The first topic). We then create a **text** variable to uniquely identify each text, with the format X.Y.Z indicating the Zth segment of 20 sentences in the Yth transcript of the Xth category. By using this approach, the number of observations increases from 286 to 1,471 and we name this data frame as **TED_full**. To sum up, **TED_full** consists of 1,471 observations with 7 variables which are posted, cate, like, view, subcate, text, transcript. 


```{r, echo=TRUE, warning=FALSE}
# Increase the number of instances: 20 sentences = 1 instance
TED_full <- TED[0,]
TED_full$subcate <- TED_full$cate #new col but same type
TED_full$text <- TED_full$cate
n_transcript <- length(TED$tanscript)

sub_cate_1 = 0
sub_cate_2 = 0
sub_cate_3 = 0

for (i in 1:n_transcript) {
  
  if (TED$cate[i] == "1") {
    sub_cate_1 <- sub_cate_1 + 1
    subcat_temp =  paste(TED$cate[i],".",as.character(sub_cate_1), sep = "", collapse = "")
  } 
  else if (TED$cate[i] == "2") {
    sub_cate_2 <- sub_cate_2 + 1
    subcat_temp =  paste(TED$cate[i],".",as.character(sub_cate_2), sep = "", collapse = "")
  }
  else {
    sub_cate_3 <- sub_cate_3 + 1
    subcat_temp =  paste(TED$cate[i],".",as.character(sub_cate_3), sep = "", collapse = "")
  }
   
  transcript_i <- TED$tanscript[i]
  transcript_i_sentence <- unlist(tokenize_sentence(transcript_i))
  n_sen <- length(transcript_i_sentence)
  n_group <- ceiling(n_sen/20)
  for (j in 1:n_group) {
    if (j == n_group) {
      sentence_temp <- paste(transcript_i_sentence[((j-1)*20+1):(n_sen)], collapse = " ")
    } 
    else {
      sentence_temp <- paste(transcript_i_sentence[((j-1)*20+1):(j*20)], collapse = " ")
    }
    
    text_temp = paste(subcat_temp,".",as.character(j), sep = "", collapse = "")
    TED_temp <- data.frame(posted = TED$posted[i], cate = TED$cate[i], like = TED$likes[i], view = TED$views_details[i], subcate = subcat_temp, text = text_temp, tanscript = sentence_temp)
    TED_full <- rbind(TED_full, TED_temp)
  }
    
}

TED_full$tanscript <- trim_ws(TED_full$tanscript)

# Our final table = TED_full consisting of 1471 instances

kable(TED_full[10,], caption = "The example of  TED_full table") %>%
  kable_paper() %>%
  kableExtra::scroll_box(width = "100%", height = "200px")

```

## 3.2 Tokenization

We tokenize our transcript by the quanteda package aiming to receive Document-Term Matrix and TFIDF matrix. In this section, we perform the tokenization twice. First, we tokenize **TED**, which consists of 286 videos/observations, to gain access into hidden insights each video and to observe the similarity and dissimilarity of each video. Second, we tokenize **TED_full**, which consists of 1,471 instances, for unsupervised and supervised learning analyses.

### 3.2.1 Tokenization from TED 

We apply **corpus()** and **tokens()** functions to the transcript variable to remove numbers, all characters in the "punctuation", symbols, and separators. We then remove stop words from the SMART information retrieval system in English (571 words) and also delete 2 more words, applaud and laughter, that they appear often in our transcript as sound representation. Sound representation in a transcript is one of the translated functionality of TED meant to enable deaf and hard-of-hearing viewers to understand all the non-spoken auditory information. Afterward, we perform lemmatization and name the data frame as **TED.tk1**.

To obtain the Document-Term Matrix (DTM) and the TFIDF matrix, we use **dfm()** and **dfm_tfidf()** functions, respectively. The first 10 terms and 10 documents (videos) are shown below.

```{r, echo=TRUE, warning=FALSE}
# Quanteda
TED.cp1 <- corpus(TED$tanscript)
#summary(TED.cp1)

TED.tk1 <- tokens(
  TED.cp1, 
  remove_numbers = TRUE, 
  remove_punct = TRUE, 
  remove_symbols = TRUE, 
  remove_separators = TRUE)

TED.tk1 <- TED.tk1 %>% 
  tokens_tolower() %>% 
  tokens_remove(c(stopwords(source = "smart"), "applaud", "laughter"))

TED.tk1 <- tokens_replace(
  TED.tk1,
  pattern = hash_lemmas$token, 
  replacement = hash_lemmas$lemma)

TED.dfm1 <- dfm(TED.tk1)
kable(TED.dfm1[1:10,1:10], caption = "The example of Document-Term Matrix") %>%
  kable_paper() %>%
  kableExtra::scroll_box(width = "100%", height = "200px")

TED.tfidf1 <- dfm_tfidf(TED.dfm1)  
kable(TED.tfidf1[1:10,1:10], caption = "The example of TFIDF matrix") %>%
  kable_paper() %>%
  kableExtra::scroll_box(width = "100%", height = "200px")

```
<br>
Additionally, the frequencies of terms can simply be obtained using **textstat_frequency()**. The terms are ranked by their frequency (rank = 1 for the most frequent), then plotted versus its rank as shown below to illustrate Zipf's law. According to the Zipf's law, *the frequency that a word appears is inversely proportional to its rank*, we notice that the pattern of the relationships of frequency and rank follows Zipf's law.

We then present the scatter plot on a log10-log10 scale. Although we expect to see a linear relationship from this plot, we observe some deviation from Zipf's law on the right hand side of the chart (high ranks/ low frequency words). The reason of the deviation might be that the text we are analyzing is not a representative sample of the language. For example, the text might contain a lot of technical terms. In other words, there are many specific terms.

```{r, echo=TRUE, warning=FALSE ,fig.height = 4}
TED.freq1 <- textstat_frequency(TED.dfm1)
#head(TED.freq1, 10)

zipf_orig<- ggplot(TED.freq1,
       aes(x = rank, y = frequency, label = feature)) + 
  geom_point() + 
  geom_text_repel() +
  ggtitle("The relationship of frequency and rank")

zipf_log <- ggplot(TED.freq1,
       aes(x = rank, y = frequency, label = feature)) + 
  geom_point() + 
  geom_text_repel() +
  scale_x_log10() +
  scale_y_log10() +
  ggtitle("The relationship of frequency and rank on log10-log10 scale") 

(zipf_orig+zipf_log)+
  plot_layout(guides = "collect") 

```


### 3.2.2 Tokenization from TED_full for unsupervised and supervised learning analyses

In this section, we repeat the same steps of the tokenization as previous section. Afterwards, we store the table and name it as **TED.tk**. We also present the first 10 terms and 10 documents of the Document-Term Matrix and the TFIDF matrix shown below.
```{r, echo=TRUE, warning=FALSE}
# Quanteda
TED.cp <- corpus(TED_full$tanscript)
#summary(TED.cp)

TED.tk <- tokens(
  TED.cp, 
  remove_numbers = TRUE, 
  remove_punct = TRUE, 
  remove_symbols = TRUE, 
  remove_separators = TRUE)

TED.tk <- TED.tk %>% 
  tokens_tolower() %>% 
  tokens_remove(c(stopwords(source = "smart"), "applaud", "laughter"))

TED.tk <- tokens_replace(
  TED.tk,
  pattern = hash_lemmas$token, 
  replacement = hash_lemmas$lemma)

TED.dfm <- dfm(TED.tk)
kable(TED.dfm[1:10,1:10], caption = "The example of Document-Term Matrix") %>%
  kable_paper() %>%
  kableExtra::scroll_box(width = "100%", height = "200px")

TED.tfidf <- dfm_tfidf(TED.dfm)  
kable(TED.tfidf[1:10,1:10], caption = "The example of TFIDF matrix") %>%
  kable_paper() %>%
  kableExtra::scroll_box(width = "100%", height = "200px")

```





